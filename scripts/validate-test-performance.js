#!/usr/bin/env node
/**
 * Script de validation continue des performances de tests
 * Objectif: Maintenir les tests sous 30 secondes
 */

const { execSync } = require('child_process');
const fs = require('fs');

const PERFORMANCE_THRESHOLDS = {
    totalTime: 30000, // 30 secondes max
    slowTestThreshold: 5000, // 5 secondes par test
    maxFailures: 0, // Tol√©rance z√©ro pour les √©checs
    minCoverage: 70 // Couverture minimale
};

const COLORS = {
    green: '\x1b[32m',
    red: '\x1b[31m',
    yellow: '\x1b[33m',
    blue: '\x1b[34m',
    reset: '\x1b[0m'
};

function log(message, color = 'reset') {
    console.log(`${COLORS[color]}${message}${COLORS.reset}`);
}

function runTests() {
    const startTime = Date.now();
    
    try {
        log('üß™ Ex√©cution des tests avec monitoring de performance...', 'blue');
        
        // Ex√©cuter les tests avec configuration bulletproof
        const result = execSync('jest --config jest.config.bulletproof.js --passWithNoTests', {
            encoding: 'utf8',
            timeout: PERFORMANCE_THRESHOLDS.totalTime + 5000 // Buffer de 5s
        });
        
        const endTime = Date.now();
        const totalTime = endTime - startTime;
        
        return {
            success: true,
            totalTime,
            output: result
        };
        
    } catch (error) {
        const endTime = Date.now();
        const totalTime = endTime - startTime;
        
        return {
            success: false,
            totalTime,
            error: error.message,
            output: error.stdout || ''
        };
    }
}

function analyzeResults(testResult) {
    const { success, totalTime, output, error } = testResult;
    
    log('\nüìä Analyse des r√©sultats de performance:', 'blue');
    
    // Temps total
    const timeStatus = totalTime <= PERFORMANCE_THRESHOLDS.totalTime ? 'green' : 'red';
    log(`‚è±Ô∏è  Temps total: ${(totalTime / 1000).toFixed(2)}s (seuil: ${PERFORMANCE_THRESHOLDS.totalTime / 1000}s)`, timeStatus);
    
    // Succ√®s des tests
    const successStatus = success ? 'green' : 'red';
    log(`‚úÖ Statut: ${success ? 'R√âUSSI' : '√âCHEC'}`, successStatus);
    
    // Analyse de l'output
    if (output) {
        const lines = output.split('\n');
        
        // Extraire les statistiques Jest
        const testStats = lines.find(line => line.includes('Tests:'));
        if (testStats) {
            log(`üìã ${testStats.trim()}`, success ? 'green' : 'red');
        }
        
        // Rechercher les tests lents
        const slowTests = lines.filter(line => 
            line.includes('ms') && 
            parseInt(line.match(/(\d+)ms/)?.[1] || 0) > PERFORMANCE_THRESHOLDS.slowTestThreshold
        );
        
        if (slowTests.length > 0) {
            log('‚ö†Ô∏è  Tests lents d√©tect√©s:', 'yellow');
            slowTests.forEach(test => log(`   ${test.trim()}`, 'yellow'));
        }
    }
    
    if (error) {
        log(`‚ùå Erreur: ${error}`, 'red');
    }
    
    return {
        passed: success && totalTime <= PERFORMANCE_THRESHOLDS.totalTime,
        totalTime,
        success
    };
}

function generateReport(results) {
    const report = {
        timestamp: new Date().toISOString(),
        performance: {
            totalTime: results.totalTime,
            passed: results.passed,
            threshold: PERFORMANCE_THRESHOLDS.totalTime
        },
        tests: {
            success: results.success
        }
    };
    
    // Sauvegarder le rapport
    const reportPath = './test-performance-report.json';
    fs.writeFileSync(reportPath, JSON.stringify(report, null, 2));
    
    log(`\nüìÑ Rapport sauvegard√©: ${reportPath}`, 'blue');
    
    return report;
}

function main() {
    log('üöÄ Validation continue des performances de tests', 'blue');
    log('‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê', 'blue');
    
    const testResult = runTests();
    const analysis = analyzeResults(testResult);
    const report = generateReport(analysis);
    
    log('\nüéØ R√©sum√© final:', 'blue');
    
    if (analysis.passed) {
        log('‚úÖ VALIDATION R√âUSSIE - Tests bulletproof maintenus!', 'green');
        log(`   Temps: ${(analysis.totalTime / 1000).toFixed(2)}s / ${PERFORMANCE_THRESHOLDS.totalTime / 1000}s`, 'green');
    } else {
        log('‚ùå VALIDATION √âCHOU√âE', 'red');
        
        if (!analysis.success) {
            log('   - Tests en √©chec d√©tect√©s', 'red');
        }
        
        if (analysis.totalTime > PERFORMANCE_THRESHOLDS.totalTime) {
            log(`   - D√©passement de temps: ${(analysis.totalTime / 1000).toFixed(2)}s / ${PERFORMANCE_THRESHOLDS.totalTime / 1000}s`, 'red');
        }
        
        log('\nüîß Actions recommand√©es:', 'yellow');
        log('   1. Identifier les tests lents avec npm test -- --verbose', 'yellow');
        log('   2. Optimiser les mocks et setup', 'yellow');
        log('   3. Parall√©liser les tests si n√©cessaire', 'yellow');
        log('   4. V√©rifier les fuites m√©moire et handles ouverts', 'yellow');
    }
    
    // Exit code pour CI/CD
    process.exit(analysis.passed ? 0 : 1);
}

if (require.main === module) {
    main();
}

module.exports = { runTests, analyzeResults, generateReport };